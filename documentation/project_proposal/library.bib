Automatically generated by Mendeley 1.0.1
Any changes to this file will be lost if it is regenerated by Mendeley.

@inproceedings{Newton:2011,
abstract = {This paper examines the creation of augmented musical instruments by a number of musicians. Equipped with a system called the Augmentalist, 10 musicians created new augmented instruments based on their traditional acoustic or electric instruments. This paper discusses the ways in which the musicians augmented their instruments, examines the similarities and differences between the resulting instruments and presents a number of interesting findings resulting from this process.},
address = {Oslo, Norway},
author = {Newton, Dan and Marshall, Mark},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Jensenius, Alexander Refsum and Tveit, Anders and God\o y, Rolf Inge and Overholt, Dan},
file = {:Users/Simon/Daten/Studium/Mendeley/2011/Newton, Marshall/Newton, Marshall - 2011 - Examining How Musicians Create Augmented Musical Instruments.pdf:pdf},
issn = {2220-4806},
keywords = { Digital Musical Instruments, Instrument Design, Performance,Augmented Instruments},
pages = {155--160},
title = {{Examining How Musicians Create Augmented Musical Instruments}},
url = {http://www.nime2011.org/proceedings/papers/D03-Newton.pdf},
year = {2011}
}
@article{Fels2011,
author = {Fels, Sidney and Lyons, Michael},
file = {:Users/Simon/Daten/Studium/Mendeley/2011/Fels, Lyons/Fels, Lyons - 2011 - Siggraph 2011 Course Notes Advances in New Interfaces for Musical Expression.pdf:pdf},
journal = {Notes},
title = {{Siggraph 2011 Course Notes Advances in New Interfaces for Musical Expression}},
year = {2011}
}
@inproceedings{Gurevich2010,
author = {Gurevich, Michael and Stapleton, Paul and Marquez-Borbon, Adnan},
booktitle = {NIME},
file = {:Users/Simon/Daten/Studium/Mendeley/2010/Gurevich, Stapleton, Marquez-Borbon/Gurevich, Stapleton, Marquez-Borbon - 2010 - Style and Constraint in Electronic Musical Instruments.pdf:pdf},
keywords = {design,interaction,performance,persuasive technology},
number = {Nime},
pages = {106--111},
title = {{Style and Constraint in Electronic Musical Instruments}},
year = {2010}
}
@inproceedings{Cassinelli2010,
author = {Cassinelli, A. and Kuribara, Y and Zerroug, A and Ishikawa, M and Manabe, D.},
booktitle = {NIME},
file = {:Users/Simon/Daten/Studium/Mendeley/2010/Cassinelli et al/Cassinelli et al. - 2010 - scoreLight playing with a human-sized laser pick-up.pdf:pdf},
keywords = {2,5,and humanities,arts,h,h5,interaction styles,j,methodologies and techniques,music computing,performing arts,sound and,user interfaces},
number = {Nime},
pages = {144--149},
title = {{scoreLight: playing with a human-sized laser pick-up}},
url = {http://www.educ.dab.uts.edu.au/nime/PROCEEDINGS/papers/Paper G1-G5/P144\_Cassinelli.pdf},
year = {2010}
}
@inproceedings{Miyama2010,
author = {Miyama, Chikashi},
booktitle = {NIME},
file = {:Users/Simon/Daten/Studium/Mendeley/2010/Miyama/Miyama - 2010 - Peacock A Non-haptic 3D Performance Interface.pdf:pdf},
keywords = {computer music,hardware and software design,musical interface,sensor technologies},
number = {Nime},
pages = {380--382},
title = {{Peacock : A Non-haptic 3D Performance Interface}},
url = {http://www.educ.dab.uts.edu.au/nime/PROCEEDINGS/papers/Demo O1-O20/P380\_Miyama.pdf},
year = {2010}
}
@inproceedings{Skogstad2010,
author = {Skogstad, A and Jensenius, Alexander R and Nymoen, Kristian},
booktitle = {NIME},
file = {:Users/Simon/Daten/Studium/Mendeley/2010/Skogstad, Jensenius, Nymoen/Skogstad, Jensenius, Nymoen - 2010 - Using IR Optical Marker Based Motion Capture for Exploring Musical Interaction St ale.pdf:pdf},
number = {Nime},
pages = {407--410},
title = {{Using IR Optical Marker Based Motion Capture for Exploring Musical Interaction St ale}},
url = {http://www.educ.dab.uts.edu.au/nime/PROCEEDINGS/papers/Demo O1-O20/P407\_Skogstad.pdf},
year = {2010}
}
@inproceedings{Friberg:2011,
abstract = {This is an overview of the three installations Hoppsa Universum, CLOSE and Flying Carpet. They were all designed as choreographed sound and music installations controlled by the visitors movements. The perspective is from an artistic goal/vision intention in combination with the technical challenges and possibilities. All three installations were realized with video cameras in the ceiling registering the users' position or movement. The video analysis was then controlling different types of interactive software audio players. Different aspects like narrativity, user control, and technical limitations are discussed.},
address = {Oslo, Norway},
author = {Friberg, Anders and K\"{a}llblad, Anna},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Jensenius, Alexander Refsum and Tveit, Anders and God\o y, Rolf Inge and Overholt, Dan},
file = {:Users/Simon/Daten/Studium/Mendeley/2011/Friberg, K\"{a}llblad/Friberg, K\"{a}llblad - 2011 - Experiences from video-controlled sound installations.pdf:pdf},
issn = {2220-4806},
keywords = { choreography, dance, interactive music, music installation,Gestures},
pages = {128--131},
title = {{Experiences from video-controlled sound installations}},
url = {http://www.nime2011.org/proceedings/papers/B26-Friberg.pdf},
year = {2011}
}
@inproceedings{Berdahl:2011,
abstract = {The purpose of this brief paper is to revisit the question of longevity in present experimental practice and coin the term autonomous new media artefacts (AutoNMA), which are complete and independent of external computer systems, so they can be operable for a longer period of time and can be demonstrated at a moment's notice. We argue that platforms for prototyping should promote the creation of AutoNMA to make extant the devices which will be a part of the future history of new media.},
address = {Oslo, Norway},
author = {Berdahl, Edgar and Chafe, Chris},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Jensenius, Alexander Refsum and Tveit, Anders and God\o y, Rolf Inge and Overholt, Dan},
file = {:Users/Simon/Daten/Studium/Mendeley/2011/Berdahl, Chafe/Berdahl, Chafe - 2011 - Autonomous New Media Artefacts (AutoNMA).pdf:pdf},
issn = {2220-4806},
keywords = { Arduino, Satellite CCRMA, standalone,autonomous},
pages = {322--323},
title = {{Autonomous New Media Artefacts (AutoNMA)}},
url = {http://www.nime2011.org/proceedings/papers/H02-Berdahl.pdf},
year = {2011}
}
@inproceedings{Hahnel2010,
author = {H\"{a}hnel, Tilo and Berndt, Axel},
booktitle = {NIME},
file = {:Users/Simon/Daten/Studium/Mendeley/2010/H\"{a}hnel, Berndt/H\"{a}hnel, Berndt - 2010 - Expressive articulation for synthetic music performances.pdf:pdf},
keywords = {articulation,expressive performance,historically informed},
number = {Nime},
pages = {277--282},
title = {{Expressive articulation for synthetic music performances}},
url = {http://isgwww.cs.uni-magdeburg.de/visual/files/publications/2010/Haehnel\_2010\_NIMEa.pdf},
year = {2010}
}
@inproceedings{Torre2010,
author = {Torre, Giuseppe and Leary, Mark O and Tuohy, Brian},
booktitle = {NIME},
file = {:Users/Simon/Daten/Studium/Mendeley/2010/Torre, Leary, Tuohy/Torre, Leary, Tuohy - 2010 - POLLEN A Multimedia Interactive Network Installation.pdf:pdf},
keywords = {3d physics emulator,computer labs,design,educational tools,installation,interactive,network,public spaces,site-specific art,sound},
number = {Nime},
pages = {375--376},
title = {{POLLEN A Multimedia Interactive Network Installation}},
url = {http://www.educ.dab.uts.edu.au/nime/PROCEEDINGS/papers/Demo N1-N20/P375\_Torre.pdf},
year = {2010}
}
@inproceedings{Zappi2010,
author = {Zappi, Victor and Italiano, Istituto and Brogni, Andrea and Caldwell, Darwin},
booktitle = {NIME},
file = {:Users/Simon/Daten/Studium/Mendeley/2010/Zappi et al/Zappi et al. - 2010 - OSC Virtual Controller.pdf:pdf},
keywords = {con-,glove device,music controller,osc,virtual reality},
number = {Nime},
pages = {297--302},
title = {{OSC Virtual Controller}},
url = {http://www.educ.dab.uts.edu.au/nime/PROCEEDINGS/papers/Paper M1-M4/P297\_Zappi.pdf},
year = {2010}
}
@inproceedings{O'Keefe2011,
author = {O'Keefe, Patrick and Essl, Georg},
booktitle = {New Interfaces for Musical Express NIME},
file = {:Users/Simon/Daten/Studium/Mendeley/2011/O'Keefe, Essl/O'Keefe, Essl - 2011 - The Visual in Mobile Music Performance.pdf:pdf},
keywords = {camera phone,mo,mobile performance,visual interaction},
pages = {191--196},
title = {{The Visual in Mobile Music Performance}},
year = {2011}
}
@inproceedings{Havryliv2010,
author = {Havryliv, Mark},
booktitle = {NIME},
file = {:Users/Simon/Daten/Studium/Mendeley/2010/Havryliv/Havryliv - 2010 - Composing For Improvisation with Chaotic Oscillators.pdf:pdf},
keywords = {audio de,chaos music,chaotic dynamics oscillators,dif,ferential equations music,mathematica,scriptors mpeg 7},
number = {Nime},
pages = {94--99},
title = {{Composing For Improvisation with Chaotic Oscillators}},
url = {http://www.educ.dab.uts.edu.au/nime/PROCEEDINGS/papers/Paper E1-E5/P94\_Havryliv.pdf},
year = {2010}
}
@inproceedings{Caramiaux:2011a,
abstract = {This paper presents a prototypical tool for sound selection driven by users' gestures. Sound selection by gestures is a particular case of ''query by content'' in multimedia databases. Gesture-to-Sound matching is based on computing the similarity between both gesture and sound parameters' temporal evolution. The tool presents three algorithms for matching gesture query to sound target. The system leads to several applications in sound design, virtual instrument design and interactive installation.},
address = {Oslo, Norway},
author = {Caramiaux, Baptiste and Bevilacqua, Frederic and Schnell, Norbert},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Jensenius, Alexander Refsum and Tveit, Anders and God\o y, Rolf Inge and Overholt, Dan},
file = {:Users/Simon/Daten/Studium/Mendeley/2011/Caramiaux, Bevilacqua, Schnell/Caramiaux, Bevilacqua, Schnell - 2011 - Sound Selection by Gestures.pdf:pdf},
issn = {2220-4806},
keywords = { Sonic Interaction, Time Series Analysis,Query by Gesture},
pages = {329--330},
title = {{Sound Selection by Gestures}},
url = {http://www.nime2011.org/proceedings/papers/H05-Caramiaux.pdf},
year = {2011}
}
@inproceedings{Snyder:2011,
abstract = {The Snyderphonics Manta controller is a USB touch controller for music and video. It features 48 capacitive touch sensors, arranged in a hexagonal grid, with bi-color LEDs that are programmable from the computer. The sensors send continuous data proportional to surface area touched, and a velocitydetection algorithm has been implemented to estimate attack velocity based on this touch data. In addition to these hexagonal sensors, the Manta has two high-dimension touch sliders (giving 12-bit values), and four assignable function buttons. In this paper, I outline the features of the controller, the available methods for communicating between the device and a computer, and some current uses for the controller.},
address = {Oslo, Norway},
author = {Snyder, Jeff},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Jensenius, Alexander Refsum and Tveit, Anders and God\o y, Rolf Inge and Overholt, Dan},
file = {:Users/Simon/Daten/Studium/Mendeley/2011/Snyder/Snyder - 2011 - The Snyderphonics Manta and a Novel USB Touch Controller.pdf:pdf},
issn = {2220-4806},
keywords = {HID,Manta,Snyderphonics,USB,capacitive,controller,decoupled LED,grid,hexagon,live music,live video,portable,sensor,touch,touch slider,wood},
pages = {413--416},
title = {{The Snyderphonics Manta and a Novel USB Touch Controller}},
url = {http://www.nime2011.org/proceedings/papers/L05-Snyder.pdf},
year = {2011}
}
@inproceedings{Collins2010,
author = {Collins, Nick},
booktitle = {NIME},
file = {:Users/Simon/Daten/Studium/Mendeley/2010/Collins/Collins - 2010 - Contrary Motion An oppositional interactive music system.pdf:pdf},
keywords = {beat tracking,contrary,musical agent,stream analysis},
number = {Nime},
pages = {125--129},
title = {{Contrary Motion: An oppositional interactive music system}},
url = {http://www.educ.dab.uts.edu.au/nime/PROCEEDINGS/papers/Paper F1-F5/P125\_Collins.pdf},
year = {2010}
}
@inproceedings{Milne:2011,
abstract = {In this paper, we describe a playable musical interface for tablets and multi-touch tables. The interface is a generalized keyboard, inspired by the Thummer, and consists of an array of virtual buttons. On a generalized keyboard, any given interval always has the same shape (and therefore fingering); furthermore, the fingering is consistent over a broad range of tunings. Compared to a physical generalized keyboard, a virtual version has some advantages---notably, that the spatial location of the buttons can be transformed by shears and rotations, and their colouring can be changed to reflect their musical function in different scales. We exploit these flexibilities to facilitate the playing not just of conventional Western scales but also a wide variety of microtonal generalized diatonic scales known as moment of symmetry, or well-formed, scales. A user can choose such a scale, and the buttons are automatically arranged so their spatial height corresponds to their pitch, and buttons an octave apart are always vertically above each other. Furthermore, the most numerous scale steps run along rows, while buttons within the scale are light-coloured, and those outside are dark or removed. These features can aid beginners; for example, the chosen scale might be the diatonic, in which case the piano's familiar white and black colouring of the seven diatonic and five chromatic notes is used, but only one scale fingering need ever be learned (unlike a piano where every key needs a different fingering). Alternatively, it can assist advanced composers and musicians seeking to explore the universe of unfamiliar microtonal scales.},
address = {Oslo, Norway},
author = {Milne, Andrew and Xamb\'{o}, Anna and Laney, Robin and Sharp, David B and Prechtl, Anthony and Holland, Simon},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Jensenius, Alexander Refsum and Tveit, Anders and God\o y, Rolf Inge and Overholt, Dan},
file = {:Users/Simon/Daten/Studium/Mendeley/2011/Milne et al/Milne et al. - 2011 - Hex Player A Virtual Musical Controller.pdf:pdf},
issn = {2220-4806},
keywords = { iPad, isomorphic layout, microtonality, multi-touch surface, musical interface design, tablet,generalized keyboard},
pages = {244--247},
title = {{Hex Player: A Virtual Musical Controller}},
url = {http://www.nime2011.org/proceedings/papers/G09-Milne.pdf},
year = {2011}
}
@inproceedings{Schlei2010,
author = {Schlei, Kevin},
booktitle = {NIME},
file = {:Users/Simon/Daten/Studium/Mendeley/2010/Schlei/Schlei - 2010 - Relationship-Based Instrument Mapping of Multi-Point Data Streams Using a Trackpad Interface.pdf:pdf},
keywords = {instrument mapping,multi-,multi-point,multi-touch interface,point data analysis,trackpad instrument},
number = {June},
pages = {136ff},
title = {{Relationship-Based Instrument Mapping of Multi-Point Data Streams Using a Trackpad Interface}},
url = {http://www.educ.dab.uts.edu.au/nime/PROCEEDINGS/papers/Paper F1-F5/P136\_Schlei.pdf},
year = {2010}
}
@misc{Lee2008,
abstract = {As of December 2007, Nintendo has sold over 20 million Wii consoles worldwide. This significantly exceeds the number of tablet PCs used today according to even the most generous estimates of tablet PC sales. This makes the Nintendo Wii remote one of the most common input devices in the world. It also happens to be one of the most sophisticated containing a 3-axis accelerometer and high-resolution high-speed infrared camera. This is an incredible opportunity to explore interaction techniques enabled by the Wii remote and to develop new applications that could be instantly accessible to millions of individuals around the world. Though only just a few weeks old, the work I will present has received nearly 5 million unique views and generated over 250,000 software downloads. In this talk, I will show you how you can participate in these projects as well as generate your own.},
author = {Lee, Johnny Chung},
booktitle = {Applied Sciences},
file = {:Users/Simon/Daten/Studium/Mendeley/2008/Lee/Lee - 2008 - Interaction Techniques Using The Wii Remote.pdf:pdf},
publisher = {Carnegie Mellon University},
title = {{Interaction Techniques Using The Wii Remote}},
year = {2008}
}
@inproceedings{Schnell:2011,
abstract = {We are presenting a set of applications that have been realized with the MO modular wireless motion capture device and a set of software components integrated into Max/MSP. These applications, created in the context of artistic projects, music pedagogy, and research, allow for the gestural reembodiment of recorded sound and music. They demonstrate a large variety of different ''playing techniques'' in musical performance using wireless motion sensor modules in conjunction with gesture analysis and real-time audio processing components.},
address = {Oslo, Norway},
author = {Schnell, Norbert and Bevilacqua, Frederic and Rasamimana, Nicolas and Blois, Julien and Guedy, Fabrice and Flety, Emmanuel},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Jensenius, Alexander Refsum and Tveit, Anders and God\o y, Rolf Inge and Overholt, Dan},
file = {:Users/Simon/Daten/Studium/Mendeley/2011/Schnell et al/Schnell et al. - 2011 - Playing the MO -- Gestural Control and Re-Embodiment of Recorded Sound and Music.pdf:pdf},
issn = {2220-4806},
keywords = { Audio Processing, Design, Gesture, Gesture Recognition, Interaction, Interface, Wireless Sensors,Music},
pages = {535--536},
title = {{Playing the "MO" -- Gestural Control and Re-Embodiment of Recorded Sound and Music}},
url = {http://www.nime2011.org/proceedings/papers/N05-Schnell.pdf},
year = {2011}
}
@inproceedings{Whalley2010,
author = {Whalley, Ian},
booktitle = {NIME},
file = {:Users/Simon/Daten/Studium/Mendeley/2010/Whalley/Whalley - 2010 - Generative Improv. \&amp Interactive Music Project (GIIMP).pdf:pdf},
keywords = {flocking,genetic algorithm,gesture,improvisation,interaction},
number = {Nime},
pages = {255--258},
title = {{Generative Improv. \&amp; Interactive Music Project (GIIMP)}},
url = {http://www.educ.dab.uts.edu.au/nime/PROCEEDINGS/papers/Paper L1-L5/P255\_Whalley.pdf},
year = {2010}
}
@inproceedings{Ramkissoon:2011,
abstract = {The Bass Sleeve uses an Arduino board with a combination of buttons, switches, flex sensors, force sensing resistors, and an accelerometer to map the ancillary movements of a performer to sampling, real-time audio and video processing including pitch shifting, delay, low pass filtering, and onscreen video movement. The device was created to augment the existing functions of the electric bass and explore the use of ancillary gestures to control the laptop in a live performance. In this research it was found that incorporating ancillary gestures into a live performance could be useful when controlling the parameters of audio processing, sound synthesis and video manipulation. These ancillary motions can be a practical solution to gestural multitasking allowing independent control of computer music parameters while performing with the electric bass. The process of performing with the Bass Sleeve resulted in a greater amount of laptop control, an increase in the amount of expressiveness using the electric bass in combination with the laptop, and an improvement in the interactivity on both the electric bass and laptop during a live performance. The design uses various gesture-to-sound mapping strategies to accomplish a compositional task during an electro acoustic multimedia musical performance piece.},
address = {Oslo, Norway},
author = {Ramkissoon, Izzi},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Jensenius, Alexander Refsum and Tveit, Anders and God\o y, Rolf Inge and Overholt, Dan},
file = {:Users/Simon/Daten/Studium/Mendeley/2011/Ramkissoon/Ramkissoon - 2011 - The Bass Sleeve A Real-time Multimedia Gestural Controller for Augmented Electric Bass Performance.pdf:pdf},
issn = {2220-4806},
keywords = { Augmented Instruments, Electric Bass, Gesture Controllers, Interactive Performance Systems, Video Tracking,Interactive Music},
pages = {224--227},
title = {{The Bass Sleeve: A Real-time Multimedia Gestural Controller for Augmented Electric Bass Performance}},
url = {http://www.nime2011.org/proceedings/papers/G04-Ramkissoon.pdf},
year = {2011}
}
@inproceedings{Deleflie2010,
author = {Deleflie, Etienne and Schiemer, Greg},
booktitle = {NIME},
file = {:Users/Simon/Daten/Studium/Mendeley/2010/Deleflie, Schiemer/Deleflie, Schiemer - 2010 - Images as spatial sound maps.pdf:pdf},
keywords = {ambisonics,decorrelation,diffusion,granular synthesis,spatial audio,surround sound},
number = {Nime},
pages = {130--135},
title = {{Images as spatial sound maps}},
url = {http://www.educ.dab.uts.edu.au/nime/PROCEEDINGS/papers/Paper F1-F5/P130\_Deleflie.pdf},
year = {2010}
}
@inproceedings{Nymoen2010,
author = {Nymoen, Kristian and Glette, Kyrre and Skogstad, SA},
booktitle = {NIME},
file = {:Users/Simon/Daten/Studium/Mendeley/2010/Nymoen, Glette, Skogstad/Nymoen, Glette, Skogstad - 2010 - Searching for Cross-Individual Relationships between Sound and Movement Features using an SVM Classifier.pdf:pdf},
number = {Nime},
pages = {259--262},
title = {{Searching for Cross-Individual Relationships between Sound and Movement Features using an SVM Classifier}},
url = {http://www.educ.dab.uts.edu.au/nime/PROCEEDINGS/papers/Paper L1-L5/P259\_Nymoen.pdf},
year = {2010}
}
@inproceedings{Cannon2010,
author = {Cannon, Joanne and Favilla, Stuart},
booktitle = {NIME},
file = {:Users/Simon/Daten/Studium/Mendeley/2010/Cannon, Favilla/Cannon, Favilla - 2010 - Expression and Spatial Motion Playable Ambisonics.pdf:pdf},
keywords = {augmented instruments,expressive spatial,playable instruments},
number = {Nime},
pages = {120--124},
title = {{Expression and Spatial Motion : Playable Ambisonics}},
year = {2010}
}
@inproceedings{Miller2010,
author = {Miller, Jace},
booktitle = {NIME},
file = {:Users/Simon/Daten/Studium/Mendeley/2010/Miller/Miller - 2010 - Wiiolin a virtual instrument using the Wii remote.pdf:pdf},
keywords = {cello,figure 1,gesture recognition,human computer interaction,motion,recognition,the orientation of the,violin,virtual instrument,wii remote,wii remote determines},
number = {June},
pages = {497ff},
title = {{Wiiolin: a virtual instrument using the Wii remote}},
url = {http://www.educ.dab.uts.edu.au/nime/PROCEEDINGS/papers/Demo Q1-Q15/P497\_Miller.pdf},
year = {2010}
}
@inproceedings{Dobrian2006,
abstract = {Is there a distinction between New Interfaces for Musical Expression and New Interfaces for Controlling Sound? This article begins with a brief overview of expression in musical performance, and examines some of the characteristics of effective “expressive” computer music instruments. It becomes apparent that sophisticated musical expression requires not only a good control interface but also virtuosic mastery of the instrument it controls. By studying effective acoustic instruments, choosing intuitive but complex gesture-sound mappings that take advantage of established instrumental skills, designing intelligent characterizations of performance gestures, and promoting long-term dedicated practice on a new interface, computer music instrument designers can enhance the expressive quality of computer music performance.},
annote = {"The lack of virtuosity on new musical interfaces is apparently another case of the “elephant in the corner”—a big bothersome issue that everyone knows is there but is hesitant to discuss."},
author = {Dobrian, Christopher and Koppelman, Daniel},
booktitle = {NIME},
file = {:Users/Simon/Daten/Studium/Mendeley/2006/Dobrian, Koppelman/Dobrian, Koppelman - 2006 - The E in NIME musical expression with new computer interfaces.pdf:pdf},
keywords = {expression,instrument design,performance,virtuosity},
pages = {277--282},
title = {{The E in NIME: musical expression with new computer interfaces}},
year = {2006}
}
@inproceedings{OKeefe:2011,
abstract = {Visual information integration in mobile music performance is an area that has not been thoroughly explored and current applications are often individually designed. From camera input to flexible output rendering, we discuss visual performance support in the context of urMus, a meta-environment for mobile interaction and performance development. The use of cameras, a set of image primitives, interactive visual content, projectors, and camera flashes can lead to visually intriguing performance possibilities.},
address = {Oslo, Norway},
author = {O'Keefe, Patrick and Essl, Georg},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Jensenius, Alexander Refsum and Tveit, Anders and God\o y, Rolf Inge and Overholt, Dan},
file = {:Users/Simon/Daten/Studium/Mendeley/2011/O'Keefe, Essl/O'Keefe, Essl - 2011 - The Visual in Mobile Music Performance.pdf:pdf},
issn = {2220-4806},
keywords = {Mobile performance,camera phone,mobile collaboration,visual interaction},
pages = {191--196},
title = {{The Visual in Mobile Music Performance}},
url = {http://www.nime2011.org/proceedings/papers/F03-OKeefe.pdf},
year = {2011}
}
@inproceedings{Yoo:2011,
abstract = {Recently, Microsoft introduced a game interface called Kinect for the Xbox 360 video game platform. This interface enables users to control and interact with the game console without the need to touch a controller. It largely increases the users' degree of freedom to express their emotion. In this paper, we first describe the system we developed to use this interface for sound generation and controlling musical expression. The skeleton data are extracted from users' motions and the data are translated to pre-defined MIDI data. We then use the MIDI data to control several applications. To allow the translation between the data, we implemented a simple Kinect-to-MIDI data convertor, which is introduced in this paper. We describe two applications to make music with Kinect: we first generate sound with Max/MSP, and then control the adlib with our own adlib generating system by the body movements of the users.
},
address = {Oslo, Norway},
author = {Yoo, Min-Joon and Beak, Jin-Wook and Lee, In-Kwon},
booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
editor = {Jensenius, Alexander Refsum and Tveit, Anders and God\o y, Rolf Inge and Overholt, Dan},
file = {:Users/Simon/Daten/Studium/Mendeley/2011/Yoo, Beak, Lee/Yoo, Beak, Lee - 2011 - Creating Musical Expression using Kinect.pdf:pdf},
issn = {2220-4806},
keywords = { adlib generation, gaming interface, sound generation,Kinect},
pages = {324--325},
title = {{Creating Musical Expression using Kinect}},
url = {http://www.nime2011.org/proceedings/papers/H03-Yoo.pdf},
year = {2011}
}
@article{Godøy2006,
abstract = {Both musicians and non-musicians can often be seen making sound-producing gestures in the air without touching any real instruments. Such air playing can be regarded as an expression of how people perceive and imagine music, and studying the relationships between these gestures and sound might contribute to our knowledge of how gestures help structure our experience of music.},
author = {God\o y, Rolf Inge and Haga, Egil and Jensenius, Alexander Refsum},
file = {:Users/Simon/Daten/Studium/Mendeley/2006/God\o y, Haga, Jensenius/God\o y, Haga, Jensenius - 2006 - Playing “Air Instruments” Mimicry of Sound-Producing Gestures by Novices and Experts.pdf:pdf},
journal = {Gesture in HumanComputer Interaction and Simulation},
number = {2006},
pages = {256--267},
publisher = {Springer},
title = {{Playing “Air Instruments”: Mimicry of Sound-Producing Gestures by Novices and Experts}},
url = {http://www.springerlink.com/index/w67h353315qt1152.pdf},
volume = {3881},
year = {2006}
}
@inproceedings{Fels:2009:CNI:1667239.1667250,
address = {New York, NY, USA},
author = {Fels, Sidney and Lyons, Michael},
booktitle = {ACM SIGGRAPH 2009 Courses},
doi = {http://doi.acm.org/10.1145/1667239.1667250},
file = {:Users/Simon/Daten/Studium/Mendeley/2009/Fels, Lyons/Fels, Lyons - 2009 - Creating new interfaces for musical expression introduction to NIME.pdf:pdf},
pages = {11:1----11:158},
publisher = {ACM},
series = {SIGGRAPH '09},
title = {{Creating new interfaces for musical expression: introduction to NIME}},
url = {http://doi.acm.org/10.1145/1667239.1667250},
year = {2009}
}
